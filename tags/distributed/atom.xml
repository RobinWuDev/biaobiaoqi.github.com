<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: distributed | Biaobiaoqi的博客]]></title>
  <link href="http://biaobiaoqi.github.com/tags/distributed/atom.xml" rel="self"/>
  <link href="http://biaobiaoqi.github.com/"/>
  <updated>2013-05-22T02:41:09+08:00</updated>
  <id>http://biaobiaoqi.github.com/</id>
  <author>
    <name><![CDATA[Biaobiaoqi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop和RDBMS的混合系统介绍]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/20/hybrid-distributed-data-management-system/"/>
    <updated>2013-05-20T14:52:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/20/hybrid-distributed-data-management-system</id>
    <content type="html"><![CDATA[<p>现在大数据概念被时常提起，社会各界对其关注度越来越高。往往越是火热的东西，人们越容易忽略它的本质。在slides中，我首先按照自己的理解，简单的理顺数据处理领域的发展历程。之后，落脚点是两个比较有代表性的混合的分布式系统：<a href="http://biaobiaoqi.me/blog/2013/05/18/a-hybrid-system-hadoopdb/">HadoopDB</a>和微软的<a href="http://biaobiaoqi.me/blog/2013/04/25/split-querying-process-in-polybase/">Polybase</a>。由于缺乏实战经验，很多东西由各方论文和博文中得到，有不恰当的地方，欢迎大家拍砖讨论;)</p>

<p>slides的提纲如下：</p>

<!--more-->


<h2>提纲</h2>

<h3>背景</h3>

<ul>
<li>RDBMS的出现</li>
<li>大数据时代到来</li>
<li>NoSQL技术</li>
<li>新时代的挑战</li>
</ul>


<h3>HadoopDB</h3>

<ul>
<li>PB级数据分析</li>
<li>HadoopDB是什么</li>
<li>框架和组件介绍</li>
<li>示例</li>
<li>总结</li>
</ul>


<h3>Polybase</h3>

<ul>
<li>Polybase总览</li>
<li>PDW结构</li>
<li>Polybase的实现</li>
<li>性能分析</li>
</ul>


<p>slides在线展示：</p>

<script async class="speakerdeck-embed" data-id="77bdc950a3460130c98a12e3c5740641" data-ratio="1.33333333333333" src="http://biaobiaoqi.github.com//speakerdeck.com/assets/embed.js"></script>


<p>slides下载：
<a href="https://dl.dropboxusercontent.com/u/64021093/slides/Hybrid%20system.pdf">请戳这里</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式一致性]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/18/distributed-consistency/"/>
    <updated>2013-05-18T18:58:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/18/distributed-consistency</id>
    <content type="html"><![CDATA[<p>本文来自<a href="http://book.douban.com/subject/3108801/">《分布式原理与泛型》</a>的一致性章节笔记。由于缺乏实践经验，这本书对我来说太过理论，难于理解，现在已经暂停该书的阅读，转而加强实践。另有相关博文<a href="http://biaobiaoqi.me/blog/2013/05/15/cap-and-eventual-consistent/">《CAP和最终一致性》</a>，可供参考阅读。</p>

<!--more-->


<h3>1.分布式的一致性概述</h3>

<p>分布式系统的一个重要问题是数据的复制。对数据的复制一般有两个原因：</p>

<ul>
<li>1.增加系统的可靠性，防止单点失效的问题；</li>
<li>2.提高系统性能，利用不同地理位置的副本迅速响应用户需求。</li>
</ul>


<p>数据复制的主要难题是保持各个副本的一致性。即在更新一个副本时，必须确保同时更新其他的副本，否则数据的各个副本将不再相同。</p>

<h3>2.以数据为中心的一致性模型</h3>

<p>一致性模型实质上是进程和数据存储之间的一个约定。正常情况下，一个数据项上执行读操作时，它期待该操作返回的是该数据在其最后一次写操作之后的结果。在没有全局时钟的情况下，精确的定义哪次写操作是最后一次写操作是十分困难的。于是就产生了一系列用其他方式定义的一致性模型。</p>

<h4>2.1 持续一致性</h4>

<p>有人提定义了区分不一致性的三个互相独立的坐标轴：副本之间的数值偏差，副本之间新旧程度偏差以及更新操作顺序的偏差。</p>

<p>数值偏差可以这样理解：已应用于其他的副本，但还没有应用于给定副本的更新数目。</p>

<p>文中使用了<a href="http://en.wikipedia.org/wiki/Vector_clock">向量时钟</a>来举例对一致性单元进行持续抑制性分析，</p>

<h4>2.2 严格一致性</h4>

<p>任意read(x)操作都要读到最新的write(x)的结果。
依赖于绝对的全局时钟，实际系统不可能做到。</p>

<h4>2.3顺序一致性</h4>

<p>对于一些读写写操作的集合，所有进程看到的都是同样的顺序。也就是将并行的操作序列化，而且每个进程得到的序列都相同。</p>

<p>那么，很自然对于同一个进程执行的操作，必然要按照它们执行的顺序出现。</p>

<h4>2.4因果一致性（Casual Consitency）</h4>

<p>要求：</p>

<p>有因果关系的写操作必须按照它们的因果关系的顺序被看到，没有因果关系的写操作可以以任意顺序被别的进程看到。</p>

<p>例如：(其中[....]是占位符，表示没有操作)</p>

<blockquote><p>进程1  W(x)a 将x写成a
进程2  [..]R(x)a W(x)b
进程3  [....]R(x)a R(x)b
进程4  [....]R(x)b R(x)a</p></blockquote>

<p>进程3和1、2是满足因果一致性的，加上4就不满足了。因为进程2是由于读到x=a才把x写成b的，所以W(x)a和W(x)b之间有因果关系，必须按照因果的顺序出现。</p>

<p>相比顺序一致性，去掉了那些没有联系的操作达成一致顺序观点的要求，只是保留那些必要的顺序（有因果关系的）。</p>

<h4>2.5入口一致性（Entry Consistency）</h4>

<p>其实也就是对每个共享数据定义一个同步变量（即：锁）。当然，没有进行同步就进行的读操作结果是不保证的。</p>

<h3>3 客户为中心的一致性</h3>

<p>也就是从用户视角来看数据是一致的。只是关心数据最终会一致（eventually consitent）。</p>

<p>只要保证对于同一个用户，他访问到的数据是一致的就可以了。如果用户只是访问一个副本，这个就很好实现，否则就需要一定策略了。当没有更多的更新的时候，要保证当前的更新会最终传播到所有的副本上。著名的例子有：DNS系统，万维网。</p>

<p>但最终一致性需要注意一个典型的问题。即当客户访问不同的副本时，问题就出现了。更具体的例子比如，博客作者更改了一篇博文内容，在A地的用户先访问到最新的内容，而B地由于离博客服务器远，看到的还是原先的内容。</p>

<p>对于最终一致性的的数据存储而言，这个示例很有代表性。问题是由用户有时可能对不同的副本进行操作的事实引起的。以客户为中心的一致性分为如下几大类：</p>

<h4>3.1 单调读（Monotonic Reads）</h4>

<p>当进程从一个地方读出数据x，那么这个以后再读到的x应该是和当前x相同或比当前更新的版本。也就是如果进程迁移到了别的位置，那么对x的更新应该比进程先到达。这里的客户就是指这个进程。</p>

<h4>3.2 单调写（Monotonic Writes）</h4>

<p>跟单调读相应，如果一个进程写一个数据x，那么它在本地或者迁移到别的地方再进行写操作的时候，原来的写操作必须要先传播到这个位置。也就是进程要在任何地方至少和上一次写一样新的数据。</p>

<h4>3.3 Read your writes</h4>

<p>一个进程对于数据x的写操作，那么进程无论到任何副本上都应该看到这个写操作的影响，也就是看到和自己写操作的影响或者更新的值。</p>

<h4>3.4 Writes follow reads</h4>

<p>顾名思义了，也就是在读操作后面的写操作要是基于至少跟上一次读出来一样新的值。也就是如果进程在地点1读了x，那么在地点2要写x的副本的话，至少写的时候应该是基于至少和地点1读出的一样新的值。</p>

<h3>4 副本放置（Replica Placement）</h3>

<h4>4.1 放置的三个方法</h4>

<p>Permanent replica/永久副本: 选几个固定位置放置副本，镜像呈现。</p>

<p>Server-initiated/服务端发起: 服务端动态决定什么时候向什么地方分发副本，又称push cache</p>

<p>Client-initiated/客户端发起: 客户端缓存,client cache</p>

<h4>4.2 更新传播</h4>

<h5>4.2.1 传播什么？</h5>

<ul>
<li>可以是更新的通知（客户自己来取）</li>
<li>可以是更新后的数据</li>
<li>可以是更新的操作</li>
</ul>


<h5>4.2.2 谁来传播</h5>

<ul>
<li>服务器push或客户pull</li>
</ul>


<h5>4.2.3 传播协议？</h5>

<ul>
<li>保证最终会收敛到一致</li>
</ul>


<h4>4.3 复制协议（Replication Protocols）</h4>

<h5>4.3.1 远程写（Remote-Write）协议</h5>

<p>对于数据x有一个主副本，当没有x副本的服务器操作x的时候就会得到一个x的副本。而有x副本的服务器响应客户读请求的时候可以立刻返回。而当客户进行写操作的时候，写操作首先在主副本完成，然后再通知其他副本更新。</p>

<h5>4.3.2 本地写（Local-Write）协议</h5>

<p>和远程写比较类似，不同点是当一个服务器得到了副本以后就成了新的主副本，以后的写操作首先在本地完成，然后再通知其他的副本，本地写的名字由此而来。</p>

<h5>4.3.3 主动复制（Active Replication）</h5>

<p>对于副本的操作要设计到另外一个单一对象。比如，n个副本代理对象C的一个接口，如果向所有副本发出请求，每个副本都会发一个请求到C，那么同样的操作就执行了n次。所以需要一个协调者。另外，多副本返回值的时候也会有同样的情况。所以这种只执行一次的动作需要副本种有一个协调者，保证操作只被执行一次或者只返回一次。</p>

<h5>3.3.4 基于候选团（Quorum-Based）协议</h5>

<p>也就是读操作要得到r个服务器的同意，写操作要得到w个进程的同意。总共有n个进程。r和w要满足以下条件：</p>

<ul>
<li>r＋w > n 这样就不可能同时发生读写，并且读到的server肯定有一个以上被更新过的</li>
<li>w > n/2   这样就不可能同时发生两个写</li>
</ul>


<p>也就是类似于投票获得读写的锁。在Dynamo系统中可以配置这种wrn参数以自持特定的一致性。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HadoopDB：混合分布式系统]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/18/a-hybrid-system-hadoopdb/"/>
    <updated>2013-05-18T17:58:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/18/a-hybrid-system-hadoopdb</id>
    <content type="html"><![CDATA[<p>HadoopDB是一个Mapreduce和传统关系型数据库的结合方案，以充分利用RDBMS的性能和Hadoop的容错、分布特性。2009年被Yale大学教授Abadi提出，继而商业化为<a href="http://hadapt.com/">Hadapt</a>，据称从VC那儿拉到了10M刀投资。</p>

<p>本文是对HadoopDB论文的总结。其中不免掺杂些自己的不成熟想法，更详细的内容，还请参见原论文 HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads</p>

<!--more-->


<h2>背景</h2>

<h3>PB级数据分析系统的能力要求</h3>

<ul>
<li>1.性能：节省开销（时间、资金）。</li>
<li>2.容错：数据分析系统（即使有故障节点也能顺利工作） 不同于 事务型的系统的容错（从故障中无损的恢复）。节点故障时，原来的查询操作不需要重启。</li>
<li>3.在异构型环境中运行的能力。即使所有机器硬件一样，但某些机器在某些时候可能因为软件原因、网络原因也会性能降低。分布式操作时，要防止木桶效应。</li>
<li>4.活的查询接口：商业化的数据分析一般建立在SQL查询上，UDF等non-SQL也是需要的。</li>
</ul>


<h5>并行数据库</h5>

<p>满足1,4：利用分表的方式，扩散到多个节点。一般情况下节点最多为几十个，原因：1.每增加一个节点，失败率增加；2.并行数据库假设各个机器都是同质化的，但这往往不太可能</p>

<h5>MapReduce</h5>

<p>满足2,3,4：Map - repartition - Reduce原为非结构化数据，但也可以适用结构化数据。</p>

<ul>
<li>2：（错误节点）动态的规划节点执行任务，将错误节点任务发放给新节点。并在本地磁盘做checkpoint存储。</li>
<li>3：（拖后腿的节点）节点间冗余的执行。执行慢的节点的任务交付给速度快的节点执行</li>
<li>4：Hive的HQL</li>
</ul>


<h5>HadoopDB</h5>

<p>融合了之前两者，做出系统层面的改进，而不仅仅是语言和接口层面。</p>

<p>这三个解决方案对4个指标的关系如下图：</p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/hadoopDB/QQ%E6%88%AA%E5%9B%BE20130518135802.png" title="compare" alt="alt compare" /></p>

<h2>架构</h2>

<p>如图
<img src="https://dl.dropboxusercontent.com/u/64021093/hadoopDB/QQ%E6%88%AA%E5%9B%BE20130518135814.png" title="framework" alt="alt framework" /></p>

<h2>组件介绍</h2>

<h5>Databse Connector:</h5>

<ul>
<li><p>作用</p>

<p>  hadoopTask &lt;-通信-> Database on Node。节点上的DB类似于Hadoop中的数据源HDFS</p></li>
<li><p>实现</p>

<p>  扩展了Hadoop的InputFormat</p></li>
</ul>


<h5>Catalog：</h5>

<ul>
<li><p>作用</p>

<p>  1.链接参数如数据库位置，驱动类和证书；
  2.一些元数据如数据簇中的数据集，副本的位置，数据的划分。</p></li>
<li><p>实现</p>

<p>  HDFS上的XML。希望做成类似于Hadoop的namenode。</p></li>
</ul>


<h5>Data Loader</h5>

<ul>
<li><p>作用</p>

<p>  将数据合理划分，从HDFS转移到节点中的本地文件系统</p></li>
<li><p>实现</p>

<p>  global hasher：分配到不同节点
  local hasher：继续划分为不同chunks</p></li>
</ul>


<h5>SQL to MapReduce to SQL (SMS) Planner</h5>

<ul>
<li><p>作用</p>

<p>  将HiveQL转化为特定执行计划，在hadoopDB中执行。原则是尽可能的讲操作推向节点上的RDBMS上执行，以此提高执行效率。</p></li>
<li><p>实现</p>

<p>  扩展Hive：
  1.执行查找前，用catolog的信息更新Hive的metastore，定向到节点数据库的表
  2.执行前，决定划分的键；将部分查询语句推到节点的数据库中执行。</p></li>
</ul>


<h2>示例</h2>

<p>示例参见下文的slides</p>

<h2>总结</h2>

<p>对hadoopDB的一些看法：</p>

<ul>
<li>其数据预处理代价过高：数据需要进行两次分解和一次数据库加载操作后才能使用；</li>
<li>将查询推向数据库层只是少数情况，大多数情况下，查询仍由Ｈive完成．因为数据仓库查询往往涉及多表连接，由于连接的复杂性，难以做到在保持连接数据局部性的前提下将参与连接的多张表按照某种模式划分；</li>
<li>维护代价过高．不仅要维护Ｈadoop系统，还要维护每个数据库节点；</li>
<li>目前尚不支持数据的动态划分，需要手工一次划分好</li>
</ul>


<p>slides：</p>

<script async class="speakerdeck-embed" data-id="48c5e680a1ab0130e1707290244918d4" data-ratio="1.33333333333333" src="http://biaobiaoqi.github.com//speakerdeck.com/assets/embed.js"></script>


<p>下载slides，请猛戳<a href="https://dl.dropboxusercontent.com/u/64021093/hadoopDB/%5B2013-05-18%5DHadoopDB.pptx">这里</a></p>

<h2>参考资料</h2>

<ul>
<li>HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads</li>
<li><a href="http://dbanotes.net/database/hadoopdb.html">《HadoopDB》 —— Fenng</a></li>
<li>《架构大数据:挑战、现状与展望》 计算机学报 王珊</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CAP和最终一致性]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/15/cap-and-eventual-consistent/"/>
    <updated>2013-05-15T00:30:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/15/cap-and-eventual-consistent</id>
    <content type="html"><![CDATA[<p>查阅资料整理了最终一致性、CAP相关的内容。由于图省事儿，没有做文字的整理记载，只有slides和一些查阅过的链接，大家将就着看。欢迎指正。</p>

<p>slides：</p>

<script async class="speakerdeck-embed" data-id="cca07ce09e92013076c646310b996896" data-ratio="1.33333333333333" src="http://biaobiaoqi.github.com//speakerdeck.com/assets/embed.js"></script>




<!--more-->


<p>slides链接：<a href="https://speakerdeck.com/biaobiaoqi/cap-and-eventually-consistent">请戳这里</a></p>

<h2>背景</h2>

<p>为什么系统要扩张？历史的发展路径是怎么样的？请看<a href="http://rdc.taobao.com/blog/cs/?p=614">《系统可扩展性演化》</a></p>

<h2>CAP理论</h2>

<ul>
<li><p>CAP理论的提出：分布式系统的CAP理论是2000年左右被提出的概念，直到Dynamo的出现，开始在工业界被广泛实践：
<a href="http://www.julianbrowne.com/article/viewer/brewers-cap-theorem">《Brewer's CAP Theorem》</a>/<a href="http://code.alibabatech.com/blog/dev_related_728/brewers-cap-theorem.html">中文翻译</a></p></li>
<li><p>对CAP的理解：<a href="http://www.douban.com/group/topic/11765014/">《谈正确理解CAP理论》</a>\ <a href="http://rdc.taobao.com/blog/cs/?p=631">《CAP理论及分布式系统一致性》</a></p></li>
</ul>


<h2>BASE理论</h2>

<ul>
<li>BASE的理论解释：<a href="http://rdc.taobao.com/blog/cs/?p=637">《分布式事务工程实现》</a></li>
</ul>


<h2>最终一致性</h2>

<ul>
<li><p>amazon的CTO分析最终一致性：<a href="http://www.allthingsdistributed.com/2008/12/eventually_consistent.html">Eventually Consistent - Revisited</a>/<a href="http://blog.csdn.net/xiaoqiangxx/article/details/7566654">中文翻译</a></p></li>
<li><p>Dynamo论文：<a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></p>

<p>关于Dynamo的一篇中文简述：<a href="http://www.infoq.com/cn/articles/nosql-dynamo">《解读NoSQL技术代表之作Dynamo》</a></p></li>
</ul>


<h2>引申</h2>

<ul>
<li>CAP原作者对CAP的反思和澄清：<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed">《CAP十二年回顾：规则变了》</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[全分布式的Hadoop初体验]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/12/touch-hadoop/"/>
    <updated>2013-05-12T00:26:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/12/touch-hadoop</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>之前的时间里对Hadoop的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。</p>

<h2>配置</h2>

<ul>
<li>内存:8G</li>
<li>CPU：i5-2400 3.1GHz；</li>
<li>硬盘：960G</li>
<li>系统：windows 7旗舰 64bits</li>
</ul>


<!--more-->


<ul>
<li>虚拟机：VMware7.1.1</li>
<li>虚拟集群：</li>
<li><ul>
<li>T （master节点）Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T2（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T3（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T4（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
</ul>


<h2>环境准备</h2>

<h5>1.节点机器的配置</h5>

<p>配置固定IP:修改<code>/etc/nerwork/interfaces</code>
<code>
auto lo
iface lo inet loopback
address 192.168.108.131
gateway 192.168.108.2
netmask 192.168.108.0
broadcast 192.168.108.0
</code></p>

<p>为了便于管理，建议按统一约定修改hostname：修改<code>/etc/hostname</code>；同时，Hadoop集群要求每个节点使用同一个账号来管理、运行，所以，也需要设置好公用账号。</p>

<h5>2.集群ssh配置</h5>

<p>ssh相关原理和操作，参见博文<a href="http://biaobiaoqi.me/blog/2013/04/19/use-ssh/">《SSH原理和使用》</a>。</p>

<p>在每台机器上生成密钥对，并将所有机器的公钥集成到master的<code>~/.ssh/authorized_keys</code>中,之后将这个文件分发到集群所有机器上。
这样，所有机器之间都可以实现免密码的ssh访问了。</p>

<p>使用如下指令，可以将本机的公钥添加到master的authorized_keys文件末尾。当所有节点都执行一遍以后，再将master的authorized_keys发布到各个节点上。
```</p>

<h1>cat .ssh/id_rsa.pub | ssh T 'cat >> ~/.ssh/authorized_keys'</h1>

<p>```</p>

<h5>3.工具脚本</h5>

<p>在分布式的环境里，运维工作的自动化很有必要。为了方便集群的运维，我写了两个简单的batch脚本。</p>

<h6>统一执行脚本</h6>

<p>在所有节点上执行同样的动作。使用时，在master节点上调用batch脚本，参数为对应的batch执行语句。</p>

<p>```</p>

<h1>!/bin/bash</h1>

<h1>Program:</h1>

<h1>Execute instructions in hosts in slaveslist.</h1>

<h1>Description:</h1>

<h1>2013/5/8 biaobiaoqi First Release</h1>

<p>if [ $# -lt 1 ]; then
   echo  "usage: $0 COMMAND"
   exit 0
fi</p>

<p>for i in <code>cat slaveslist</code>
do
   ssh biaobiaoqi@$i "$1"
done</p>

<p>```</p>

<p>脚本中使用的slaveslist文件保存着所有slave节点的hostname，需要与脚本放在同一个工作目录下。</p>

<h6>统一替部署脚本</h6>

<p>将主节点的某文件或目录统一的更新部署替换到所有节点上（注意，所有节点拥有相同的目录结构，即替换的文件路径相同）。</p>

<p>遇到hadoop集群中节点的增删改动需要修改配置文件的，都可以通过这个脚本便捷的部署。</p>

<p>```</p>

<h1>!/bin/bash</h1>

<h1>Program:</h1>

<h1>Put the dirctory into all nodes of the cluster as the same path.</h1>

<h1>Description:</h1>

<h1>2013/5/10     biaobiaoqi     First Release</h1>

<p>if [ $# -lt 1 ]; then
   echo "Usage $0 DIR_PATH"
   exit 0
fi</p>

<p>for i in <code>cat slaveslist</code>
do
   ssh $i "rm ~/tmp -rf"
   scp -r $1 $i:~/tmp
   ssh $i "rm -rf $1;  mv ~/tmp $1"
done</p>

<p>```</p>

<h5>4.配置hosts文件</h5>

<p>由于hadoop体系在处理节点时，是使用的hostname，而非IP，所以必须先配置好hostname和IP的关系。
在一台机器上修改<code>/etc/hosts</code>
```</p>

<h1>/etc/hosts</h1>

<p>127.0.0.1     localhost
192.168.108.128     T3
192.168.108.129     T2
192.168.108.130 T
192.168.108.131 T4
```
然后使用统一执行脚本，将它发布到所有节点上。</p>

<p>值得注意的是，在<code>/etc/hostsname</code>中修改了host name之后，如果不同步的修改<code>/etc/hosts</code>中的相关信息，则在sudo操作时出现 <code>sudo: unable to resolve host</code>  的提示。原因是机器无法解析主机名。</p>

<p>修改<code>/etc/hosts</code>时也要特别注意，如果改成<code>127.0.0.1 localhost HOSTNAME</code> (其中HOSTNAME是主机名)的形式，在开启hadoop集群时，会出现datanode无法正常访问namenode，算是个小bug吧。所以得把hosts文件写成如上的形式。</p>

<h5>5.配置Java环境</h5>

<p>Hadoop需要Java1.6或更高版本，记住Java的安装目录，之后需要在hadoop配置过程中用到。</p>

<h2>安装Hadoop</h2>

<h5>1.下载Hadoop</h5>

<p>从官网下载<a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Hadoop发布版</a>（博主使用的是较早的稳定版0.20.2）</p>

<p>关于版本选择，推荐阅读：<a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">Hadoop版本选择探讨</a></p>

<h5>2.部署</h5>

<p>解压下载好的Hadoop，后放到合适的目录下。这里假定放置在/home/USER/ 的目录下</p>

<p>在<code>/home/USER/.bashrc</code>(其中USER为集群的用户名)文件中，增加如下语句，设定Hadoop相关的路径信息：
<code>
export JAVA_HOME=/usr/lib/jvm/java-6-openjdk
export HADOOP_HOME=/home/hadoop/Hadoop
export HADOOP_CONF=$HADOOP_HOME/conf
export HADOOP_PATH=$HADOOP_HOME/bin
export PATH=$HADOOP_PATH:$PATH
export CLASSPATH=.:$JAVA_HOME/bin:$PATH:$HADOOP_HOME:$HADOOP_HOME/bin
</code></p>

<h6>Hadoop核心配置修改</h6>

<p>配置文件在<code>$HADOOP_HOME/conf</code>目录下，其中基础配置比较重要的有三个：core-site.xml, hdfs-site.xml, mapred-site.xml。（当然，每个配置文件都有其细节作用，不过在初步实践hadoop时，理解这三个配置文件中的几个重要配置项就够了）</p>

<p>一般的，有三种可选模式。即本地模式、伪分布式模式和全分布式模式。前两种只是在单机环境下，后一种才是生产环境下的常用方式。《Hadoop权威指南》和《Hadoop实战》等书中都有讲到不同方式的配置，这里博主仅描述实验环境下4节点的全分布式配置。</p>

<p>core-site.xml整个hadoop的顶层配置
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;
     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
     &lt;value&gt;/home/biaobiaoqi/UDMS/hadoop-data/tmp-base&lt;/value&gt;
     &lt;description&gt;
    存放临时目录的路径，默认也被用来存储hdfs的元数据和文件数据，值得注意的是，hadoop账户对所设定的本地路径是否有足够的操作权限。之后再hdfs-site.xml中设定的dfs.data.dir和dfs.name.dir也要注意同样的问题
    &lt;/description&gt; 
&lt;/property&gt; 

 &lt;property&gt;   
      &lt;name&gt;fs.default.name&lt;/name&gt; 
      &lt;value&gt;hdfs://T:9000/&lt;/value&gt;
      &lt;description&gt;
    默认文件系统的标记。这个URI标记了文件系统的实现方式。UIR的协议决定了文件系统的实现类，而后面的值决定了文件系统的地址、端口等信息。
    &lt;/description&gt;
 &lt;/property&gt; 
</code></pre>

<p></configuration></p>

<p>```</p>

<p>hdfs-site.xml存储HDFS相关的信息</p>

<p>```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;   
      &lt;name&gt;dfs.replication&lt;/name&gt;   
      &lt;value&gt;3&lt;/value&gt;   
      &lt;description&gt;默认的块的副本数量。实际的副本数量可以在文件写入的时候确定，默认的副本数则是在没有指定写入副本时被使用。 &lt;/description&gt; 
 &lt;/property&gt;
&lt;property&gt;
     &lt;name&gt;dfs.name.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/hadoop-data/meta-data&lt;/value&gt;
    &lt;description&gt;
    设定hdfs的元数据信息存储地址。在namenode上。
    &lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
      &lt;name&gt;dfs.data.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/hadoop-data/data&lt;/value&gt;
    &lt;description&gt;
    设定hdfs的数据存储地址。在datanode上。
    &lt;/description&gt;
 &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>mapred-site.xml存储mapreduce作业相关配置
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;   
      &lt;name&gt;mapred.job.tracker&lt;/name&gt;
      &lt;value&gt;T:9001&lt;/value&gt;   
      &lt;description&gt; Mapreduce 的job tracker所在的节点和端口。&lt;/description&gt; 
 &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>hosts文件存储了master节点</p>

<p><code>
T
</code></p>

<p>slaves文件存储着所有的slaves节点
<code>
T2
T3
T4
</code></p>

<h2>启动集群</h2>

<h5>1.格式化namenode</h5>

<p>如果是第一次起动集群，需要先格式化HDFS。</p>

<p>namenode存放了HDFS的元数据，故可以看成是对HDFS的格式化。</p>

<p><code>
$HADOOP_HOME/bin/hadoop namenode -format
</code></p>

<h5>2.启动守护进程</h5>

<p><code>
$HADOOP_HOME/bin/start-all.sh
</code>
等价于如下命令执行：
```</p>

<h1>start dfs daemons</h1>

<p>$"$bin"/start-dfs.sh --config $HADOOP_CONF_DIR</p>

<h1>start mapred daemons</h1>

<p>$"$bin"/start-mapred.sh --config $HADOOP_CONF_DIR</p>

<p>```
如果成功，打开 http://T:50070 (T为集群master节点)，可以看到HDFS的运行情况，包括节点数量、空间大小等。这是Hadoop自带的HDFS监控页面；同样的，http://T:50030 是Mapreduce的监控界面。</p>

<p>如果没有成功，根据$HADOOP_HOME/logs目录下的日志文件信息debug。</p>

<h5>3.常见问题</h5>

<ul>
<li>namenode无法启动：</li>
<li><ul>
<li>删除掉本地文件系统中HDFS的目录文件，重新格式化HDFS。</li>
</ul>
</li>
<li><ul>
<li>HDFS目录的权限不够，更改权限设置等。</li>
</ul>
</li>
<li>namenode启动成功，datanode无法连接：检查hosts文件是否设置正确；检查各个配置文件中地址值是否使用了IP而不是hostname。</li>
<li>namenode启动成功，datanode无法启动：Incompatible namespaceIDs，由于频繁格式化，造成dfs.name.dir/current/VERSION与dfs.data.dir/current/VERSION数据不一致。</li>
<li>SafeModeException： 分布式系统启动时，会进入安全模式，安全模式下，hadoop是无法执行的。一般的等待一会儿，就可以正常使用了。如果是由于之前集群崩溃造成的无法自动退出安全模式的情况，则需要如下特殊处理了
<code>
$/$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave
</code></li>
</ul>


<h2>初体验</h2>

<p>最简单的尝试就是使用Hadoop自带的wordcount程序了，参照<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html">这篇文章</a>，描述很详细。</p>

<p>其他的一些尝试： <a href="http://www.cnblogs.com/rilley/archive/2012/02/13/2349858.html">动态增删节点</a> 、 <a href="http://www.cnblogs.com/ggjucheng/archive/2012/04/18/2454696.html">修改备份数量</a></p>

<h2>参考</h2>

<p><a href="http://hadoop.apache.org/docs/stable/cluster_setup.html">offical document: Cluster Setup</a></p>
]]></content>
  </entry>
  
</feed>
