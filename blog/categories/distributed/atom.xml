<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: distributed | Biaobiaoqi的博客]]></title>
  <link href="http://biaobiaoqi.github.com/blog/categories/distributed/atom.xml" rel="self"/>
  <link href="http://biaobiaoqi.github.com/"/>
  <updated>2013-05-15T23:55:40+08:00</updated>
  <id>http://biaobiaoqi.github.com/</id>
  <author>
    <name><![CDATA[Biaobiaoqi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[CAP和最终一致性]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/15/cap-and-eventual-consistent/"/>
    <updated>2013-05-15T00:30:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/15/cap-and-eventual-consistent</id>
    <content type="html"><![CDATA[<p>查阅资料整理了最终一致性、CAP相关的内容。由于图省事儿，没有做文字的整理记载，只有slides和一些查阅过的链接，大家将就着看。欢迎指正。</p>

<p>slides：</p>

<script async class="speakerdeck-embed" data-id="cca07ce09e92013076c646310b996896" data-ratio="1.33333333333333" src="http://biaobiaoqi.github.com//speakerdeck.com/assets/embed.js"></script>




<!--more-->


<p>slides链接：<a href="https://speakerdeck.com/biaobiaoqi/cap-and-eventually-consistent">请戳这里</a></p>

<h2>背景</h2>

<p>为什么系统要扩张？历史的发展路径是怎么样的？请看<a href="http://rdc.taobao.com/blog/cs/?p=614">《系统可扩展性演化》</a></p>

<h2>CAP理论</h2>

<ul>
<li><p>CAP理论的提出：分布式系统的CAP理论是2000年左右被提出的概念，直到Dynamo的出现，开始在工业界被广泛实践：
<a href="http://www.julianbrowne.com/article/viewer/brewers-cap-theorem">《Brewer's CAP Theorem》</a>/<a href="http://code.alibabatech.com/blog/dev_related_728/brewers-cap-theorem.html">中文翻译</a></p></li>
<li><p>对CAP的理解：<a href="http://www.douban.com/group/topic/11765014/">《谈正确理解CAP理论》</a>\ <a href="http://rdc.taobao.com/blog/cs/?p=631">《CAP理论及分布式系统一致性》</a></p></li>
</ul>


<h2>BASE理论</h2>

<ul>
<li>BASE的理论解释：<a href="http://rdc.taobao.com/blog/cs/?p=637">《分布式事务工程实现》</a></li>
</ul>


<h2>最终一致性</h2>

<ul>
<li><p>amazon的CTO分析最终一致性：<a href="http://www.allthingsdistributed.com/2008/12/eventually_consistent.html">Eventually Consistent - Revisited</a>/<a href="http://blog.csdn.net/xiaoqiangxx/article/details/7566654">中文翻译</a></p></li>
<li><p>Dynamo论文：<a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></p>

<p>关于Dynamo的一篇中文简述：<a href="http://www.infoq.com/cn/articles/nosql-dynamo">《解读NoSQL技术代表之作Dynamo》</a></p></li>
</ul>


<h2>引申</h2>

<ul>
<li>CAP原作者对CAP的反思和澄清：<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed">《CAP十二年回顾：规则变了》</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[全分布式的Hadoop初体验]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/05/12/touch-hadoop/"/>
    <updated>2013-05-12T00:26:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/05/12/touch-hadoop</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>之前的时间里对Hadoop的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。</p>

<h2>配置</h2>

<ul>
<li>内存:8G</li>
<li>CPU：i5-2400 3.1GHz；</li>
<li>硬盘：960G</li>
<li>系统：windows 7旗舰 64bits</li>
</ul>


<!--more-->


<ul>
<li>虚拟机：VMware7.1.1</li>
<li>虚拟集群：</li>
<li><ul>
<li>T （master节点）Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T2（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T3（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T4（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
</ul>


<h2>环境准备</h2>

<h5>1.节点机器的配置</h5>

<p>配置固定IP:修改<code>/etc/nerwork/interfaces</code>
<code>
auto lo
iface lo inet loopback
address 192.168.108.131
gateway 192.168.108.2
netmask 192.168.108.0
broadcast 192.168.108.0
</code></p>

<p>为了便于管理，建议按统一约定修改hostname：修改<code>/etc/hostname</code>；同时，Hadoop集群要求每个节点使用同一个账号来管理、运行，所以，也需要设置好公用账号。</p>

<h5>2.集群ssh配置</h5>

<p>ssh相关原理和操作，参见博文<a href="http://biaobiaoqi.me/blog/2013/04/19/use-ssh/">《SSH原理和使用》</a>。</p>

<p>在每台机器上生成密钥对，并将所有机器的公钥集成到master的<code>~/.ssh/authorized_keys</code>中,之后将这个文件分发到集群所有机器上。
这样，所有机器之间都可以实现免密码的ssh访问了。</p>

<p>使用如下指令，可以将本机的公钥添加到master的authorized_keys文件末尾。当所有节点都执行一遍以后，再将master的authorized_keys发布到各个节点上。
```</p>

<h1>cat .ssh/id_rsa.pub | ssh T 'cat >> ~/.ssh/authorized_keys'</h1>

<p>```</p>

<h5>3.工具脚本</h5>

<p>在分布式的环境里，运维工作的自动化很有必要。为了方便集群的运维，我写了两个简单的batch脚本。</p>

<h6>统一执行脚本</h6>

<p>在所有节点上执行同样的动作。使用时，在master节点上调用batch脚本，参数为对应的batch执行语句。</p>

<p>```</p>

<h1>!/bin/bash</h1>

<h1>Program:</h1>

<h1>Execute instructions in hosts in slaveslist.</h1>

<h1>Description:</h1>

<h1>2013/5/8 biaobiaoqi First Release</h1>

<p>if [ $# -lt 1 ]; then
   echo  "usage: $0 COMMAND"
   exit 0
fi</p>

<p>for i in <code>cat slaveslist</code>
do
   ssh biaobiaoqi@$i "$1"
done</p>

<p>```</p>

<p>脚本中使用的slaveslist文件保存着所有slave节点的hostname，需要与脚本放在同一个工作目录下。</p>

<h6>统一替部署脚本</h6>

<p>将主节点的某文件或目录统一的更新部署替换到所有节点上（注意，所有节点拥有相同的目录结构，即替换的文件路径相同）。</p>

<p>遇到hadoop集群中节点的增删改动需要修改配置文件的，都可以通过这个脚本便捷的部署。</p>

<p>```</p>

<h1>!/bin/bash</h1>

<h1>Program:</h1>

<h1>Put the dirctory into all nodes of the cluster as the same path.</h1>

<h1>Description:</h1>

<h1>2013/5/10     biaobiaoqi     First Release</h1>

<p>if [ $# -lt 1 ]; then
   echo "Usage $0 DIR_PATH"
   exit 0
fi</p>

<p>for i in <code>cat slaveslist</code>
do
   ssh $i "rm ~/tmp -rf"
   scp -r $1 $i:~/tmp
   ssh $i "rm -rf $1;  mv ~/tmp $1"
done</p>

<p>```</p>

<h5>4.配置hosts文件</h5>

<p>由于hadoop体系在处理节点时，是使用的hostname，而非IP，所以必须先配置好hostname和IP的关系。
在一台机器上修改<code>/etc/hosts</code>
```</p>

<h1>/etc/hosts</h1>

<p>127.0.0.1     localhost
192.168.108.128     T3
192.168.108.129     T2
192.168.108.130 T
192.168.108.131 T4
```
然后使用统一执行脚本，将它发布到所有节点上。</p>

<p>值得注意的是，在<code>/etc/hostsname</code>中修改了host name之后，如果不同步的修改<code>/etc/hosts</code>中的相关信息，则在sudo操作时出现 <code>sudo: unable to resolve host</code>  的提示。原因是机器无法解析主机名。</p>

<p>修改<code>/etc/hosts</code>时也要特别注意，如果改成<code>127.0.0.1 localhost HOSTNAME</code> (其中HOSTNAME是主机名)的形式，在开启hadoop集群时，会出现datanode无法正常访问namenode，算是个小bug吧。所以得把hosts文件写成如上的形式。</p>

<h5>5.配置Java环境</h5>

<p>Hadoop需要Java1.6或更高版本，记住Java的安装目录，之后需要在hadoop配置过程中用到。</p>

<h2>安装Hadoop</h2>

<h5>1.下载Hadoop</h5>

<p>从官网下载<a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Hadoop发布版</a>（博主使用的是较早的稳定版0.20.2）</p>

<p>关于版本选择，推荐阅读：<a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">Hadoop版本选择探讨</a></p>

<h5>2.部署</h5>

<p>解压下载好的Hadoop，后放到合适的目录下。这里假定放置在/home/USER/ 的目录下</p>

<p>在<code>/home/USER/.bashrc</code>(其中USER为集群的用户名)文件中，增加如下语句，设定Hadoop相关的路径信息：
<code>
export JAVA_HOME=/usr/lib/jvm/java-6-openjdk
export HADOOP_HOME=/home/hadoop/Hadoop
export HADOOP_CONF=$HADOOP_HOME/conf
export HADOOP_PATH=$HADOOP_HOME/bin
export PATH=$HADOOP_PATH:$PATH
export CLASSPATH=.:$JAVA_HOME/bin:$PATH:$HADOOP_HOME:$HADOOP_HOME/bin
</code></p>

<h6>Hadoop核心配置修改</h6>

<p>配置文件在<code>$HADOOP_HOME/conf</code>目录下，其中基础配置比较重要的有三个：core-site.xml, hdfs-site.xml, mapred-site.xml。（当然，每个配置文件都有其细节作用，不过在初步实践hadoop时，理解这三个配置文件中的几个重要配置项就够了）</p>

<p>一般的，有三种可选模式。即本地模式、伪分布式模式和全分布式模式。前两种只是在单机环境下，后一种才是生产环境下的常用方式。《Hadoop权威指南》和《Hadoop实战》等书中都有讲到不同方式的配置，这里博主仅描述实验环境下4节点的全分布式配置。</p>

<p>core-site.xml整个hadoop的顶层配置
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;
     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
     &lt;value&gt;/home/biaobiaoqi/UDMS/hadoop-data/tmp-base&lt;/value&gt;
     &lt;description&gt;
    存放临时目录的路径，默认也被用来存储hdfs的元数据和文件数据，值得注意的是，hadoop账户对所设定的本地路径是否有足够的操作权限。之后再hdfs-site.xml中设定的dfs.data.dir和dfs.name.dir也要注意同样的问题
    &lt;/description&gt; 
&lt;/property&gt; 

 &lt;property&gt;   
      &lt;name&gt;fs.default.name&lt;/name&gt; 
      &lt;value&gt;hdfs://T:9000/&lt;/value&gt;
      &lt;description&gt;
    默认文件系统的标记。这个URI标记了文件系统的实现方式。UIR的协议决定了文件系统的实现类，而后面的值决定了文件系统的地址、端口等信息。
    &lt;/description&gt;
 &lt;/property&gt; 
</code></pre>

<p></configuration></p>

<p>```</p>

<p>hdfs-site.xml存储HDFS相关的信息</p>

<p>```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;   
      &lt;name&gt;dfs.replication&lt;/name&gt;   
      &lt;value&gt;3&lt;/value&gt;   
      &lt;description&gt;默认的块的副本数量。实际的副本数量可以在文件写入的时候确定，默认的副本数则是在没有指定写入副本时被使用。 &lt;/description&gt; 
 &lt;/property&gt;
&lt;property&gt;
     &lt;name&gt;dfs.name.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/hadoop-data/meta-data&lt;/value&gt;
    &lt;description&gt;
    设定hdfs的元数据信息存储地址。在namenode上。
    &lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
      &lt;name&gt;dfs.data.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/hadoop-data/data&lt;/value&gt;
    &lt;description&gt;
    设定hdfs的数据存储地址。在datanode上。
    &lt;/description&gt;
 &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>mapred-site.xml存储mapreduce作业相关配置
```
&lt;?xml version="1.0"?>
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?></p>

<!-- Put site-specific property overrides in this file. -->


<p><configuration></p>

<pre><code>&lt;property&gt;   
      &lt;name&gt;mapred.job.tracker&lt;/name&gt;
      &lt;value&gt;T:9001&lt;/value&gt;   
      &lt;description&gt; Mapreduce 的job tracker所在的节点和端口。&lt;/description&gt; 
 &lt;/property&gt;
</code></pre>

<p></configuration></p>

<p>```</p>

<p>hosts文件存储了master节点</p>

<p><code>
T
</code></p>

<p>slaves文件存储着所有的slaves节点
<code>
T2
T3
T4
</code></p>

<h2>启动集群</h2>

<h5>1.格式化namenode</h5>

<p>如果是第一次起动集群，需要先格式化HDFS。</p>

<p>namenode存放了HDFS的元数据，故可以看成是对HDFS的格式化。</p>

<p><code>
$HADOOP_HOME/bin/hadoop namenode -format
</code></p>

<h5>2.启动守护进程</h5>

<p><code>
$HADOOP_HOME/bin/start-all.sh
</code>
等价于如下命令执行：
```</p>

<h1>start dfs daemons</h1>

<p>$"$bin"/start-dfs.sh --config $HADOOP_CONF_DIR</p>

<h1>start mapred daemons</h1>

<p>$"$bin"/start-mapred.sh --config $HADOOP_CONF_DIR</p>

<p>```
如果成功，打开 http://T:50070 (T为集群master节点)，可以看到HDFS的运行情况，包括节点数量、空间大小等。这是Hadoop自带的HDFS监控页面；同样的，http://T:50030 是Mapreduce的监控界面。</p>

<p>如果没有成功，根据$HADOOP_HOME/logs目录下的日志文件信息debug。</p>

<h5>3.常见问题</h5>

<ul>
<li>namenode无法启动：</li>
<li><ul>
<li>删除掉本地文件系统中HDFS的目录文件，重新格式化HDFS。</li>
</ul>
</li>
<li><ul>
<li>HDFS目录的权限不够，更改权限设置等。</li>
</ul>
</li>
<li>namenode启动成功，datanode无法连接：检查hosts文件是否设置正确；检查各个配置文件中地址值是否使用了IP而不是hostname。</li>
<li>namenode启动成功，datanode无法启动：Incompatible namespaceIDs，由于频繁格式化，造成dfs.name.dir/current/VERSION与dfs.data.dir/current/VERSION数据不一致。</li>
<li>SafeModeException： 分布式系统启动时，会进入安全模式，安全模式下，hadoop是无法执行的。一般的等待一会儿，就可以正常使用了。如果是由于之前集群崩溃造成的无法自动退出安全模式的情况，则需要如下特殊处理了
<code>
$/$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave
</code></li>
</ul>


<h2>初体验</h2>

<p>最简单的尝试就是使用Hadoop自带的wordcount程序了，参照<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html">这篇文章</a>，描述很详细。</p>

<p>其他的一些尝试： <a href="http://www.cnblogs.com/rilley/archive/2012/02/13/2349858.html">动态增删节点</a> 、 <a href="http://www.cnblogs.com/ggjucheng/archive/2012/04/18/2454696.html">修改备份数量</a></p>

<h2>参考</h2>

<p><a href="http://hadoop.apache.org/docs/stable/cluster_setup.html">offical document: Cluster Setup</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PDW中的Split Querying Process]]></title>
    <link href="http://biaobiaoqi.github.com/blog/2013/04/25/split-querying-process-in-polybase/"/>
    <updated>2013-04-25T22:59:00+08:00</updated>
    <id>http://biaobiaoqi.github.com/blog/2013/04/25/split-querying-process-in-polybase</id>
    <content type="html"><![CDATA[<p>最近看了关于SQL Server的分布式处理方面的论文，觉得它提出的Polybase跟之前看过的HadoopDB有些神似，这里做个小总结（抽空再把HadoopDB的总结贴出来）。</p>

<p>不算翻译，只是挑出自己认为是重点的部分。详细情况，还请论文查阅原文，引用中有写明出处。文章末尾有我总结的slides，可以辅助查阅。</p>

<p>由于缺乏实践经验，很多东西未必能理解其本质。如有其他观点，还请多指教。</p>

<p>当下的计划就是开始自己搭环境，实践起来!~</p>

<!--more-->


<h2>背景</h2>

<p>商业应用中，越来越多的需要将结构化数据和非结构化数据存储、处理混合起来。
目前，已经有很多公司、产品致力于这部分的研究，微软发的这篇论文，也正是基于PDW V2的这一新功能提出的新的解决方案。</p>

<h3>Polybase简介：</h3>

<p>是SQL Server PDW V2的一个新功能：通过使用SQL来管理和查询hadoop集群中的数据。
它同时能处理结构化和非结构化的数据，特点是结合了HDFS的外部表，使用基于开销的查询优化器来做分裂查询处理。</p>

<h2>相关研究</h2>

<ul>
<li><p>sqoop：用于在hadoop和结构化数据9比如关系型数据库之间传输数据。</p></li>
<li><p>teradata&amp; Asterdata&amp; Greenplum&amp; Vertica：通过外部表（external table）实现基于SQL的对hadoop中所存数据的操作。</p></li>
<li><p>Orable：基本机制也是建立外部表；另外还开发了用于加载hadoop的大数据到Oracle自家数据库的工具OLH(Oracle loader for Hadoop)。</p></li>
<li><p>IBM、Netezza:使用mapreduce方法获取分布式环境下各个节点的数据执行处理。</p></li>
<li><p>Hadapt: (HadoopDB)：HadoopDB是来自耶鲁大学的创意，并商业化为Hadapt项目。这是首个提出使用类SQL语言、集合Hadoop系统实现对RDBMS的操作的想法。实现相对简单，源代码3千多行，在Hadoop中对其中的几个模块做了二次开发。</p></li>
</ul>


<h2>PDW</h2>

<p>Polybase是PDW V2的一个新feature，那么，首先，让我们来看一下所谓的PDW是什么。</p>

<p>PDW是一个基于SQL Server的shared-nothing的并行数据库系统。</p>

<p>PDW（Parallel Data Warehouse）架构图：</p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image0.png"></p>

<h3>PDW系统中的组件：</h3>

<h5>节点</h5>

<ul>
<li>control node：</li>
</ul>


<p>类似于Hadoop中的master节点。运行着PDW Engine，负责：查询语法分析，优化，生成分布式执行计划DSQL，控制计划实施</p>

<ul>
<li>compute node：</li>
</ul>


<p>类似于Hadoop中的slave节点。数据存储和查询执行</p>

<h4>DMS</h4>

<p>Data Moving Service，起功能有：</p>

<ul>
<li><p>1.repartition rows of a table among SQL Server instances on PDW compute nodes</p></li>
<li><p>2.针对ODBC的类型转换。</p></li>
</ul>


<h2>Polybase</h2>

<h3>Polybase使用场景</h3>

<p>如图</p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image1.png"></p>

<p>（a）中PDW与Hadoop一起完成了数据处理任务，并输出结果；</p>

<p>（b）中处理数据后，结果直接存储到HDFS中</p>

<p>充分利用SQL Server PDW的性能优势，特别是它的基于开销的并行查询优化器和执行引擎。</p>

<h3>Polybase的环境需求</h3>

<ul>
<li>1.PDW与Hadoop集群可以重叠，也可以分离。</li>
<li>2.windows 和 linux部署的hadoop集群都支持。</li>
<li>3.支持所有标准的HDFS文件格式，包括文本、序列化文件、RCFiles。只要定义好对应的Inputformat和outputFormat，所有定制文件格式也支持。</li>
</ul>


<h3>核心组件</h3>

<ul>
<li>1.外部表：用于在PDW中实现对数据的操作语义。</li>
<li>2.HDFS Bridge：DMS中的组件，用于实现PDW节点域Hadoop的通信。</li>
<li>3.基于开销的查询优化器：将常规SQL查询语句转化为DSQL（分布式SQL查询语句），并结合集群状况（比如Hadoop和PDW集群规模，运行状况等），合理选择优化方式。</li>
</ul>


<h3>1.外部表</h3>

<p>PDW需要了解到Hadoop集群中数据的模型。于是就有了这个外部表。
实例如下：</p>

<p>创建集群：</p>

<p>```
CREATE HADOOP_CLUSTER GSL_CLUSTER</p>

<pre><code>  WITH (namenode=‘hadoop-head’,namenode_port=9000,

  jobtracker=‘hadoop-head’,jobtracker_port=9010);
</code></pre>

<p>```</p>

<p>创建文件格式：</p>

<p>```
CREATE HADOOP_FILEFORMAT TEXT_FORMAT</p>

<p>  WITH (INPUT_FORMAT=‘polybase.TextInputFormat’,</p>

<p>  OUTPUT_FORMAT = ‘polybase.TextOutputFormat’,</p>

<p>  ROW_DELIMITER = '\n', COLUMN_DELIMITER = ‘|’);</p>

<p>```</p>

<p>根据集群和文件信息，创建外部表
```
CREATE EXTERNAL TABLE hdfsCustomer</p>

<p>  ( c_custkey       bigint not null,</p>

<pre><code>c_name          varchar(25) not null,

 .......
)
</code></pre>

<p>WITH (LOCATION='/tpch1gb/customer.tbl',</p>

<p>FORMAT_OPTIONS (EXTERNAL_CLUSTER = GSL_CLUSTER,</p>

<p>EXTERNAL_FILEFORMAT = TEXT_FORMAT));
```</p>

<h3>2.HDFS Bridge</h3>

<p>结构如图：
<img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image2.png"></p>

<p>HDFS shuffle阶段：通过DMS从hadoop读取数据的阶段。
涉及到hdfs中的数据处理时，处理过程如下：</p>

<ul>
<li><p>1.跟namenode通信，获得hdfs中文件的信息</p></li>
<li><p>2.hdfs中文件信息 +  DMS实例个数 -> 每个DMS的输入文件（offset、长度） #力求负载均衡</p></li>
<li><p>3.将DMS的输入文件信息传递给各个DMS，DMS通过 openRecordReader（）方法构建RecordReader实例，直接与对应的datanode通信，获取数据，并转换为ODBC格式（有时候类型转换提前到mapreduce中以利用hadoop集群的计算能力）。读取过程中，使用了buffer机制提高效率。有时候数据会被提前到从HDFS中读出时执行，而不是到DMS中执行。这是为了充分利用hadoop集群的计算能力，节约CPU秘籍的DMS shuffle的计算。</p></li>
</ul>


<p>写入hadoop的过程与此类似。</p>

<h3>3.查询优化</h3>

<ul>
<li><p>1.PDW Parser(在PDW Engine的进程中完成)。</p></li>
<li><p>2.SQL Server Query Optimizer(在control node的SQL Server的进程中完成)：使用bottom-up的方式进行查询优化，并在合理的位置插入数据迁移的操作符（用于分布式环境的数据迁移指令），:生成查询计划，存储在Memo数据结构（http://www.benjaminnevarez.com/2012/04/inside-the-query-optimizer-memo-structure/）中 。</p></li>
<li><p>3.XML geneator(在control node的SQL Server的进程中完成)。接收Memo，并转换格式，往下传递。</p></li>
<li><p>4.Query Optimizer(在PDW Engine的进程中完成)：根据Memo生成DSQL。</p></li>
<li><p>5.基于开销的查询优化：判定是否将SQL语句推送到Hadoop中执行。</p>

<p>  考虑外部表的样本数据的直方图、集群的规模等因素...选择最优优化方案。</p></li>
</ul>


<h5>样本数据处理:</h5>

<p>定义对应外部表列的详细样本数据：</p>

<p><code>
CREATE STATISTICS hdfsCustomerStats ON
hdfsCustomer (c_custkey);
</code></p>

<p>对样本数据的处理的方式如下：</p>

<ul>
<li>1.通过DMS或者map job读取sample数据，</li>
<li>2.分发到不同的comute节点的暂存表。</li>
<li>3.每个节点分别计算直方图。</li>
<li>4.汇总直方图，存储到control node数据库的catalog中</li>
</ul>


<p>缺点是在此过程中没有利用好hadoop集群的计算能力。</p>

<h3>语义兼容</h3>

<p>涉及到Java和SQL以及之间的转换。包括这三个方面：</p>

<ul>
<li>数据类型的语义.</li>
<li>表达式的语义</li>
<li>异常处理机制</li>
</ul>


<p>例如："a+b"，其中a，b都为null，SQL结果为NULL，而Java则会抛出NullException。</p>

<p>处理原则是：能转化的类型则做好转化包装；不能转换的则标记为无法实现，仅限PDW实现。</p>

<h3>举例：</h3>

<p><code>
   SELECT count (*) from Customer
  WHERE acctbal &lt; 0
  GROUP BY nationkey
</code></p>

<p>如图所示为处理过程</p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image4.png"></p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image5.png"></p>

<p><img src="http://dl.dropboxusercontent.com/u/64021093/pdw/Image6.png"></p>

<h3>Polybase的MapReduce Join实现</h3>

<p>使用distributed hash join实现（只有equi-join能被在mapreduce中完成）</p>

<p>小表作为build side ，并被物化（materialized）到HDFS，大表作为probe side。</p>

<p>在Hadoop的Map任务中：读取物化好的build side到内存，构成hash table。</p>

<p>probe side经过hash后对比hash表，做正确的链接。</p>

<p>为了让build side置于内存中，需要计算build side的大小、每个task拥有的内存大小，task中执行其他操作需要的内存空间。
当然，build side也可能被复制多分，以提高效率。</p>

<p><a href="http://dl.dropboxusercontent.com/u/64021093/pdw/Split%20Query%20Processing%20in%20Polybase.pptx">本文演示slides下载链接，点击获取</a></p>

<h2>引用</h2>

<ul>
<li>Split Query Processing in Polybase(SIGMOD’13， June 22-27,2013,New York,USA.) Microsoft Corporation</li>
<li>Polybase:  What, Why, How(ppt) Microsoft Corporation</li>
<li>Query Optimization in Microsoft SQL Server PDW(SIGMOD'12, May 20-24,2012,Scottsdale,Arizona,USA) Microsoft Corporation</li>
</ul>

]]></content>
  </entry>
  
</feed>
