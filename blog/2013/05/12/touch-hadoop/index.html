
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>全分布式的Hadoop初体验 - Biaobiaoqi的博客</title>
  <meta name="author" content="Biaobiaoqi">

  
  <meta name="description" content="之前的时间里对Hadoop的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://biaobiaoqi.github.com/blog/2013/05/12/touch-hadoop">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Biaobiaoqi的博客" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
	<script type="text/javascript" >
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}

$(document).bind('DOMNodeInserted', function(event) {
  addBlankTargetForLinks();
});

</script>

<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F49481ec3305db999013860b0ccb3b16d' type='text/javascript'%3E%3C/script%3E"));
</script>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39900036-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Biaobiaoqi的博客</a></h1>
  
    <h2>共享、和平、自由</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:biaobiaoqi.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="搜索"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">首页</a></li>
<!--
  <li><a href="/blog/archives/">技术</a></li>
-->
  <li><a href="/blog/categories/tech/">技术</a></li>
  <li><a href="/blog/categories/life/">生活</a></li>
  <li><a href="/about">关于</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">全分布式的Hadoop初体验</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-12T00:26:00+08:00" pubdate data-updated="true">May 12<span>th</span>, 2013</time>
        



      </p>
    
  </header>


<div class="entry-content"><h2>背景</h2>

<p>之前的时间里对Hadoop的使用都是基于学长所搭建起的实验环境的，没有完整的自己部署和维护过，最近抽时间初体验了在集群环境下装机、配置、运行的全过程，梳理总结到本文中。</p>

<h2>配置</h2>

<ul>
<li>内存:8G</li>
<li>CPU：i5-2400 3.1GHz；</li>
<li>硬盘：960G</li>
<li>系统：windows 7旗舰 64bits</li>
</ul>


<!--more-->


<ul>
<li>虚拟机：VMware7.1.1</li>
<li>虚拟集群：</li>
<li><ul>
<li>T （master节点）Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T2（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T3（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
<li><ul>
<li>T4（slave节点） Ubuntu11.04 32 bits 内存512MB；硬盘100G；单核；</li>
</ul>
</li>
</ul>


<h2>环境准备</h2>

<h5>1.节点机器的配置</h5>

<p>配置固定IP:修改<code>/etc/nerwork/interfaces</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>auto lo
</span><span class='line'>iface lo inet loopback
</span><span class='line'>address 192.168.108.131
</span><span class='line'>gateway 192.168.108.2
</span><span class='line'>netmask 192.168.108.0
</span><span class='line'>broadcast 192.168.108.0</span></code></pre></td></tr></table></div></figure>


<p>为了便于管理，建议按统一约定修改hostname：修改<code>/etc/hostname</code>；同时，Hadoop集群要求每个节点使用同一个账号来管理、运行，所以，也需要设置好公用账号。</p>

<h5>2.集群ssh配置</h5>

<p>ssh相关原理和操作，参见博文<a href="http://biaobiaoqi.me/blog/2013/04/19/use-ssh/">《SSH原理和使用》</a>。</p>

<p>在每台机器上生成密钥对，并将所有机器的公钥集成到master的<code>~/.ssh/authorized_keys</code>中,之后将这个文件分发到集群所有机器上。
这样，所有机器之间都可以实现免密码的ssh访问了。</p>

<p>使用如下指令，可以将本机的公钥添加到master的authorized_keys文件末尾。当所有节点都执行一遍以后，再将master的authorized_keys发布到各个节点上。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#cat .ssh/id_rsa.pub | ssh T 'cat &gt;&gt; ~/.ssh/authorized_keys'</span></code></pre></td></tr></table></div></figure>


<h5>3.工具脚本</h5>

<p>在分布式的环境里，运维工作的自动化很有必要。为了方便集群的运维，我写了两个简单的batch脚本。</p>

<h6>统一执行脚本</h6>

<p>在所有节点上执行同样的动作。使用时，在master节点上调用batch脚本，参数为对应的batch执行语句。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>#Program:
</span><span class='line'># Execute instructions in hosts in slaveslist.
</span><span class='line'>#Description:
</span><span class='line'>#2013/5/8 biaobiaoqi First Release
</span><span class='line'>if [ $# -lt 1 ]; then
</span><span class='line'>   echo  "usage: $0 COMMAND"
</span><span class='line'>   exit 0
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>for i in `cat slaveslist`
</span><span class='line'>do
</span><span class='line'>   ssh biaobiaoqi@$i "$1"
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<p>脚本中使用的slaveslist文件保存着所有slave节点的hostname，需要与脚本放在同一个工作目录下。</p>

<h6>统一替部署脚本</h6>

<p>将主节点的某文件或目录统一的更新部署替换到所有节点上（注意，所有节点拥有相同的目录结构，即替换的文件路径相同）。</p>

<p>遇到hadoop集群中节点的增删改动需要修改配置文件的，都可以通过这个脚本便捷的部署。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>#Program:
</span><span class='line'>#    Put the dirctory into all nodes of the cluster as the same path.
</span><span class='line'>#Description:
</span><span class='line'>#2013/5/10     biaobiaoqi     First Release
</span><span class='line'>if [ $# -lt 1 ]; then
</span><span class='line'>   echo "Usage $0 DIR_PATH"
</span><span class='line'>   exit 0
</span><span class='line'>fi
</span><span class='line'>
</span><span class='line'>for i in `cat slaveslist`
</span><span class='line'>do
</span><span class='line'>   ssh $i "rm ~/tmp -rf"
</span><span class='line'>   scp -r $1 $i:~/tmp
</span><span class='line'>   ssh $i "rm -rf $1;  mv ~/tmp $1"
</span><span class='line'>done
</span></code></pre></td></tr></table></div></figure>


<h5>4.配置hosts文件</h5>

<p>由于hadoop体系在处理节点时，是使用的hostname，而非IP，所以必须先配置好hostname和IP的关系。
在一台机器上修改<code>/etc/hosts</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#/etc/hosts
</span><span class='line'>127.0.0.1     localhost
</span><span class='line'>192.168.108.128     T3
</span><span class='line'>192.168.108.129     T2
</span><span class='line'>192.168.108.130 T
</span><span class='line'>192.168.108.131 T4</span></code></pre></td></tr></table></div></figure>


<p>然后使用统一执行脚本，将它发布到所有节点上。</p>

<p>值得注意的是，在<code>/etc/hostsname</code>中修改了host name之后，如果不同步的修改<code>/etc/hosts</code>中的相关信息，则在sudo操作时出现 <code>sudo: unable to resolve host</code>  的提示。原因是机器无法解析主机名。</p>

<p>修改<code>/etc/hosts</code>时也要特别注意，如果改成<code>127.0.0.1 localhost HOSTNAME</code> (其中HOSTNAME是主机名)的形式，在开启hadoop集群时，会出现datanode无法正常访问namenode，算是个小bug吧。所以得把hosts文件写成如上的形式。</p>

<h5>5.配置Java环境</h5>

<p>Hadoop需要Java1.6或更高版本，记住Java的安装目录，之后需要在hadoop配置过程中用到。</p>

<h2>安装Hadoop</h2>

<h5>1.下载Hadoop</h5>

<p>从官网下载<a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Hadoop发布版</a>（博主使用的是较早的稳定版0.20.2）</p>

<p>关于版本选择，推荐阅读：<a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">Hadoop版本选择探讨</a></p>

<h5>2.部署</h5>

<p>解压下载好的Hadoop，后放到合适的目录下。这里假定放置在/home/USER/ 的目录下</p>

<p>在<code>/home/USER/.bashrc</code>(其中USER为集群的用户名)文件中，增加如下语句，设定Hadoop相关的路径信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/usr/lib/jvm/java-6-openjdk
</span><span class='line'>export HADOOP_HOME=/home/hadoop/Hadoop
</span><span class='line'>export HADOOP_CONF=$HADOOP_HOME/conf
</span><span class='line'>export HADOOP_PATH=$HADOOP_HOME/bin
</span><span class='line'>export PATH=$HADOOP_PATH:$PATH
</span><span class='line'>export CLASSPATH=.:$JAVA_HOME/bin:$PATH:$HADOOP_HOME:$HADOOP_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h6>Hadoop核心配置修改</h6>

<p>配置文件在<code>$HADOOP_HOME/conf</code>目录下，其中基础配置比较重要的有三个：core-site.xml, hdfs-site.xml, mapred-site.xml。（当然，每个配置文件都有其细节作用，不过在初步实践hadoop时，理解这三个配置文件中的几个重要配置项就够了）</p>

<p>一般的，有三种可选模式。即本地模式、伪分布式模式和全分布式模式。前两种只是在单机环境下，后一种才是生产环境下的常用方式。《Hadoop权威指南》和《Hadoop实战》等书中都有讲到不同方式的配置，这里博主仅描述实验环境下4节点的全分布式配置。</p>

<p>core-site.xml整个hadoop的顶层配置</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span><span class='line'>         &lt;value&gt;/home/biaobiaoqi/UDMS/hadoop-data/tmp-base&lt;/value&gt;
</span><span class='line'>         &lt;description&gt;
</span><span class='line'>        存放临时目录的路径，默认也被用来存储hdfs的元数据和文件数据，值得注意的是，hadoop账户对所设定的本地路径是否有足够的操作权限。之后再hdfs-site.xml中设定的dfs.data.dir和dfs.name.dir也要注意同样的问题
</span><span class='line'>        &lt;/description&gt; 
</span><span class='line'>    &lt;/property&gt; 
</span><span class='line'>
</span><span class='line'>     &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;fs.default.name&lt;/name&gt; 
</span><span class='line'>          &lt;value&gt;hdfs://T:9000/&lt;/value&gt;
</span><span class='line'>          &lt;description&gt;
</span><span class='line'>        默认文件系统的标记。这个URI标记了文件系统的实现方式。UIR的协议决定了文件系统的实现类，而后面的值决定了文件系统的地址、端口等信息。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt; 
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hdfs-site.xml存储HDFS相关的信息</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;dfs.replication&lt;/name&gt;   
</span><span class='line'>          &lt;value&gt;3&lt;/value&gt;   
</span><span class='line'>          &lt;description&gt;默认的块的副本数量。实际的副本数量可以在文件写入的时候确定，默认的副本数则是在没有指定写入副本时被使用。 &lt;/description&gt; 
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>    &lt;property&gt;
</span><span class='line'>         &lt;name&gt;dfs.name.dir&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;/home/hadoop/hadoop-data/meta-data&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;
</span><span class='line'>        设定hdfs的元数据信息存储地址。在namenode上。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>     &lt;property&gt;
</span><span class='line'>          &lt;name&gt;dfs.data.dir&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;/home/hadoop/hadoop-data/data&lt;/value&gt;
</span><span class='line'>        &lt;description&gt;
</span><span class='line'>        设定hdfs的数据存储地址。在datanode上。
</span><span class='line'>        &lt;/description&gt;
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>mapred-site.xml存储mapreduce作业相关配置</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;?xml version="1.0"?&gt;
</span><span class='line'>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
</span><span class='line'>
</span><span class='line'>&lt;!-- Put site-specific property overrides in this file. --&gt;
</span><span class='line'>
</span><span class='line'>&lt;configuration&gt; 
</span><span class='line'>    &lt;property&gt;   
</span><span class='line'>          &lt;name&gt;mapred.job.tracker&lt;/name&gt;
</span><span class='line'>          &lt;value&gt;T:9001&lt;/value&gt;   
</span><span class='line'>          &lt;description&gt; Mapreduce 的job tracker所在的节点和端口。&lt;/description&gt; 
</span><span class='line'>     &lt;/property&gt;
</span><span class='line'>&lt;/configuration&gt;
</span></code></pre></td></tr></table></div></figure>


<p>hosts文件存储了master节点</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>T</span></code></pre></td></tr></table></div></figure>


<p>slaves文件存储着所有的slaves节点</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>T2
</span><span class='line'>T3
</span><span class='line'>T4</span></code></pre></td></tr></table></div></figure>


<h2>启动集群</h2>

<h5>1.格式化namenode</h5>

<p>如果是第一次起动集群，需要先格式化HDFS。</p>

<p>namenode存放了HDFS的元数据，故可以看成是对HDFS的格式化。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$HADOOP_HOME/bin/hadoop namenode -format</span></code></pre></td></tr></table></div></figure>


<h5>2.启动守护进程</h5>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$HADOOP_HOME/bin/start-all.sh </span></code></pre></td></tr></table></div></figure>


<p>等价于如下命令执行：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># start dfs daemons
</span><span class='line'>$"$bin"/start-dfs.sh --config $HADOOP_CONF_DIR
</span><span class='line'>
</span><span class='line'># start mapred daemons
</span><span class='line'>$"$bin"/start-mapred.sh --config $HADOOP_CONF_DIR
</span></code></pre></td></tr></table></div></figure>


<p>如果成功，打开 http://T:50070 (T为集群master节点)，可以看到HDFS的运行情况，包括节点数量、空间大小等。这是Hadoop自带的HDFS监控页面；同样的，http://T:50030 是Mapreduce的监控界面。</p>

<p>如果没有成功，根据$HADOOP_HOME/logs目录下的日志文件信息debug。</p>

<h5>3.常见问题</h5>

<ul>
<li>namenode无法启动：</li>
<li><ul>
<li>删除掉本地文件系统中HDFS的目录文件，重新格式化HDFS。</li>
</ul>
</li>
<li><ul>
<li>HDFS目录的权限不够，更改权限设置等。</li>
</ul>
</li>
<li>namenode启动成功，datanode无法连接：检查hosts文件是否设置正确；检查各个配置文件中地址值是否使用了IP而不是hostname。</li>
<li>namenode启动成功，datanode无法启动：Incompatible namespaceIDs，由于频繁格式化，造成dfs.name.dir/current/VERSION与dfs.data.dir/current/VERSION数据不一致。</li>
<li>SafeModeException： 分布式系统启动时，会进入安全模式，安全模式下，hadoop是无法执行的。一般的等待一会儿，就可以正常使用了。如果是由于之前集群崩溃造成的无法自动退出安全模式的情况，则需要如下特殊处理了</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$/$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave </span></code></pre></td></tr></table></div></figure>


<h2>初体验</h2>

<p>最简单的尝试就是使用Hadoop自带的wordcount程序了，参照<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2504205.html">这篇文章</a>，描述很详细。</p>

<p>其他的一些尝试： <a href="http://www.cnblogs.com/rilley/archive/2012/02/13/2349858.html">动态增删节点</a> 、 <a href="http://www.cnblogs.com/ggjucheng/archive/2012/04/18/2454696.html">修改备份数量</a></p>

<h2>参考</h2>

<p><a href="http://hadoop.apache.org/docs/stable/cluster_setup.html">offical document: Cluster Setup</a></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Biaobiaoqi</span></span>

      








  


<time datetime="2013-05-12T00:26:00+08:00" pubdate data-updated="true">May 12<span>th</span>, 2013</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/distributed/'>distributed</a>, <a class='category' href='/blog/categories/hadoop/'>hadoop</a>, <a class='category' href='/blog/categories/linux/'>linux</a>, <a class='category' href='/blog/categories/tech/'>tech</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
  
      <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<span class="jiathis_txt">分享到：</span>
	<a class="jiathis_button_tsina">新浪微博</a>
	<a class="jiathis_button_tqq">腾讯微博</a>
	<a class="jiathis_button_renren">人人网</a>
	<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1354814218267188" charset="utf-8"></script>
<!-- JiaThis Button END -->

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" id="UYScript" src="http://v1.uyan.cc/js/iframe.js?UYUserId=1755742" async=""></script>
<!-- UY END -->

  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/05/09/networking-in-virtual-machine/" title="Previous Post: 虚拟机中的网络配置">&laquo; 虚拟机中的网络配置</a>
      
      
        <a class="basic-alignment right" href="/blog/2013/05/12/mbti-test/" title="Next Post: 我的MBTI职业性格测试">我的MBTI职业性格测试 &raquo;</a>
      
    </p>
  </footer>
</article>






</div>

<aside class="sidebar">
  
    <section>
  <h1>最近发表</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/05/12/mbti-test/">我的MBTI职业性格测试</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/12/touch-hadoop/">全分布式的Hadoop初体验</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/09/networking-in-virtual-machine/">虚拟机中的网络配置</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/27/pat1020-pat1043-rebuild-binary-tree/">根据前中后序和层序重建二叉树(PAT1020、PAT1043)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/27/travsal-binary-tree/">二叉树的遍历（递归、非递归）分析</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/biaobiaoqi">@biaobiaoqi</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'biaobiaoqi',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
<h2>我的社区</h2>
<div>
<script type="text/javascript" src="http://www.douban.com/service/badge/42071652/?show=dolist&amp;select=random&amp;n=6&amp;columns=3&amp;cat=book" ></script>
</div>
</section>
<iframe width="100%" height="550" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=0&noborder=0&isWeibo=0&isFans=0&uid=1829882343&verifier=efc8c820&dpc=1"></iframe>




  
</aside>




    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Biaobiaoqi -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
